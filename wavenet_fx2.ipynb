{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! DEPRECATED NOTEBOOK !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 4th root for embedding: https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the combined forex and economic news data\n",
    "df = pd.read_csv('fx_with_news.csv', header=[0,1], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">c_eur</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">after_counter_ohe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>bar_len</th>\n",
       "      <th>bar_spearman</th>\n",
       "      <th>bar_log_r</th>\n",
       "      <th>first_r</th>\n",
       "      <th>max_r</th>\n",
       "      <th>min_r</th>\n",
       "      <th>last_r</th>\n",
       "      <th>bar_quantile_25_r</th>\n",
       "      <th>...</th>\n",
       "      <th>_united states nondefense capital goods orders ex aircraft</th>\n",
       "      <th>_united states nonfarm payrolls</th>\n",
       "      <th>_united states retail sales control group</th>\n",
       "      <th>_united states retail sales ex autos mom</th>\n",
       "      <th>_united states retail sales mom</th>\n",
       "      <th>_united states reutersmichigan consumer sentiment index</th>\n",
       "      <th>_united states trade balance</th>\n",
       "      <th>_united states unemployment rate</th>\n",
       "      <th>_usd event</th>\n",
       "      <th>_usd speech</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:00:00</th>\n",
       "      <td>1.087467</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.737459</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:05:00</th>\n",
       "      <td>1.087362</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>155.0</td>\n",
       "      <td>-0.181051</td>\n",
       "      <td>-0.000230</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:10:00</th>\n",
       "      <td>1.086980</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-0.460522</td>\n",
       "      <td>-0.000248</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:15:00</th>\n",
       "      <td>1.086938</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.282268</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:20:00</th>\n",
       "      <td>1.087057</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-0.043127</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:35:00</th>\n",
       "      <td>1.121460</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>62.0</td>\n",
       "      <td>-0.800736</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972569</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:40:00</th>\n",
       "      <td>1.121460</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-0.141550</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000054</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:45:00</th>\n",
       "      <td>1.121388</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.612689</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:50:00</th>\n",
       "      <td>1.121327</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>97.0</td>\n",
       "      <td>-0.912678</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:55:00</th>\n",
       "      <td>1.121247</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.075062</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.971181</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298829 rows Ã— 459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        c_eur                                           \\\n",
       "                         mean       std bar_len bar_spearman bar_log_r   \n",
       "ctime                                                                    \n",
       "2016-01-03 22:00:00  1.087467  0.000015    28.0    -0.737459 -0.000037   \n",
       "2016-01-03 22:05:00  1.087362  0.000049   155.0    -0.181051 -0.000230   \n",
       "2016-01-03 22:10:00  1.086980  0.000105   138.0    -0.460522 -0.000248   \n",
       "2016-01-03 22:15:00  1.086938  0.000043   125.0     0.282268 -0.000028   \n",
       "2016-01-03 22:20:00  1.087057  0.000148   115.0    -0.043127 -0.000120   \n",
       "...                       ...       ...     ...          ...       ...   \n",
       "2019-12-31 21:35:00  1.121460  0.000037    62.0    -0.800736 -0.000080   \n",
       "2019-12-31 21:40:00  1.121460  0.000028    41.0    -0.141550 -0.000009   \n",
       "2019-12-31 21:45:00  1.121388  0.000046   108.0     0.612689 -0.000018   \n",
       "2019-12-31 21:50:00  1.121327  0.000046    97.0    -0.912678 -0.000143   \n",
       "2019-12-31 21:55:00  1.121247  0.000131    91.0     0.075062  0.000294   \n",
       "\n",
       "                                                                               \\\n",
       "                      first_r     max_r     min_r    last_r bar_quantile_25_r   \n",
       "ctime                                                                           \n",
       "2016-01-03 22:00:00  0.000030  0.000030 -0.000016 -0.000007         -0.000009   \n",
       "2016-01-03 22:05:00  0.000081  0.000081 -0.000149 -0.000149         -0.000029   \n",
       "2016-01-03 22:10:00  0.000203  0.000203 -0.000147 -0.000046         -0.000064   \n",
       "2016-01-03 22:15:00 -0.000007  0.000057 -0.000108 -0.000035         -0.000007   \n",
       "2016-01-03 22:20:00 -0.000144  0.000224 -0.000264 -0.000264         -0.000126   \n",
       "...                       ...       ...       ...       ...               ...   \n",
       "2019-12-31 21:35:00  0.000062  0.000062 -0.000036 -0.000018         -0.000027   \n",
       "2019-12-31 21:40:00 -0.000027  0.000035 -0.000054 -0.000036         -0.000018   \n",
       "2019-12-31 21:45:00  0.000029  0.000047 -0.000096  0.000011         -0.000025   \n",
       "2019-12-31 21:50:00  0.000056  0.000092 -0.000122 -0.000086         -0.000015   \n",
       "2019-12-31 21:55:00 -0.000033  0.000261 -0.000123  0.000261         -0.000078   \n",
       "\n",
       "                     ...  \\\n",
       "                     ...   \n",
       "ctime                ...   \n",
       "2016-01-03 22:00:00  ...   \n",
       "2016-01-03 22:05:00  ...   \n",
       "2016-01-03 22:10:00  ...   \n",
       "2016-01-03 22:15:00  ...   \n",
       "2016-01-03 22:20:00  ...   \n",
       "...                  ...   \n",
       "2019-12-31 21:35:00  ...   \n",
       "2019-12-31 21:40:00  ...   \n",
       "2019-12-31 21:45:00  ...   \n",
       "2019-12-31 21:50:00  ...   \n",
       "2019-12-31 21:55:00  ...   \n",
       "\n",
       "                                                             after_counter_ohe  \\\n",
       "                    _united states nondefense capital goods orders ex aircraft   \n",
       "ctime                                                                            \n",
       "2016-01-03 22:00:00                                           0.000000           \n",
       "2016-01-03 22:05:00                                           0.000000           \n",
       "2016-01-03 22:10:00                                           0.000000           \n",
       "2016-01-03 22:15:00                                           0.000000           \n",
       "2016-01-03 22:20:00                                           0.000000           \n",
       "...                                                                ...           \n",
       "2019-12-31 21:35:00                                           0.429514           \n",
       "2019-12-31 21:40:00                                           0.429167           \n",
       "2019-12-31 21:45:00                                           0.428819           \n",
       "2019-12-31 21:50:00                                           0.428472           \n",
       "2019-12-31 21:55:00                                           0.428125           \n",
       "\n",
       "                                                     \\\n",
       "                    _united states nonfarm payrolls   \n",
       "ctime                                                 \n",
       "2016-01-03 22:00:00                             0.0   \n",
       "2016-01-03 22:05:00                             0.0   \n",
       "2016-01-03 22:10:00                             0.0   \n",
       "2016-01-03 22:15:00                             0.0   \n",
       "2016-01-03 22:20:00                             0.0   \n",
       "...                                             ...   \n",
       "2019-12-31 21:35:00                             0.0   \n",
       "2019-12-31 21:40:00                             0.0   \n",
       "2019-12-31 21:45:00                             0.0   \n",
       "2019-12-31 21:50:00                             0.0   \n",
       "2019-12-31 21:55:00                             0.0   \n",
       "\n",
       "                                                               \\\n",
       "                    _united states retail sales control group   \n",
       "ctime                                                           \n",
       "2016-01-03 22:00:00                                       0.0   \n",
       "2016-01-03 22:05:00                                       0.0   \n",
       "2016-01-03 22:10:00                                       0.0   \n",
       "2016-01-03 22:15:00                                       0.0   \n",
       "2016-01-03 22:20:00                                       0.0   \n",
       "...                                                       ...   \n",
       "2019-12-31 21:35:00                                       0.0   \n",
       "2019-12-31 21:40:00                                       0.0   \n",
       "2019-12-31 21:45:00                                       0.0   \n",
       "2019-12-31 21:50:00                                       0.0   \n",
       "2019-12-31 21:55:00                                       0.0   \n",
       "\n",
       "                                                              \\\n",
       "                    _united states retail sales ex autos mom   \n",
       "ctime                                                          \n",
       "2016-01-03 22:00:00                                      0.0   \n",
       "2016-01-03 22:05:00                                      0.0   \n",
       "2016-01-03 22:10:00                                      0.0   \n",
       "2016-01-03 22:15:00                                      0.0   \n",
       "2016-01-03 22:20:00                                      0.0   \n",
       "...                                                      ...   \n",
       "2019-12-31 21:35:00                                      0.0   \n",
       "2019-12-31 21:40:00                                      0.0   \n",
       "2019-12-31 21:45:00                                      0.0   \n",
       "2019-12-31 21:50:00                                      0.0   \n",
       "2019-12-31 21:55:00                                      0.0   \n",
       "\n",
       "                                                     \\\n",
       "                    _united states retail sales mom   \n",
       "ctime                                                 \n",
       "2016-01-03 22:00:00                             0.0   \n",
       "2016-01-03 22:05:00                             0.0   \n",
       "2016-01-03 22:10:00                             0.0   \n",
       "2016-01-03 22:15:00                             0.0   \n",
       "2016-01-03 22:20:00                             0.0   \n",
       "...                                             ...   \n",
       "2019-12-31 21:35:00                             0.0   \n",
       "2019-12-31 21:40:00                             0.0   \n",
       "2019-12-31 21:45:00                             0.0   \n",
       "2019-12-31 21:50:00                             0.0   \n",
       "2019-12-31 21:55:00                             0.0   \n",
       "\n",
       "                                                                             \\\n",
       "                    _united states reutersmichigan consumer sentiment index   \n",
       "ctime                                                                         \n",
       "2016-01-03 22:00:00                                                0.0        \n",
       "2016-01-03 22:05:00                                                0.0        \n",
       "2016-01-03 22:10:00                                                0.0        \n",
       "2016-01-03 22:15:00                                                0.0        \n",
       "2016-01-03 22:20:00                                                0.0        \n",
       "...                                                                ...        \n",
       "2019-12-31 21:35:00                                                0.0        \n",
       "2019-12-31 21:40:00                                                0.0        \n",
       "2019-12-31 21:45:00                                                0.0        \n",
       "2019-12-31 21:50:00                                                0.0        \n",
       "2019-12-31 21:55:00                                                0.0        \n",
       "\n",
       "                                                  \\\n",
       "                    _united states trade balance   \n",
       "ctime                                              \n",
       "2016-01-03 22:00:00                          0.0   \n",
       "2016-01-03 22:05:00                          0.0   \n",
       "2016-01-03 22:10:00                          0.0   \n",
       "2016-01-03 22:15:00                          0.0   \n",
       "2016-01-03 22:20:00                          0.0   \n",
       "...                                          ...   \n",
       "2019-12-31 21:35:00                          0.0   \n",
       "2019-12-31 21:40:00                          0.0   \n",
       "2019-12-31 21:45:00                          0.0   \n",
       "2019-12-31 21:50:00                          0.0   \n",
       "2019-12-31 21:55:00                          0.0   \n",
       "\n",
       "                                                                             \n",
       "                    _united states unemployment rate _usd event _usd speech  \n",
       "ctime                                                                        \n",
       "2016-01-03 22:00:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:05:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:10:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:15:00                              0.0   0.000000         0.0  \n",
       "2016-01-03 22:20:00                              0.0   0.000000         0.0  \n",
       "...                                              ...        ...         ...  \n",
       "2019-12-31 21:35:00                              0.0   0.972569         0.0  \n",
       "2019-12-31 21:40:00                              0.0   0.972222         0.0  \n",
       "2019-12-31 21:45:00                              0.0   0.971875         0.0  \n",
       "2019-12-31 21:50:00                              0.0   0.971528         0.0  \n",
       "2019-12-31 21:55:00                              0.0   0.971181         0.0  \n",
       "\n",
       "[298829 rows x 459 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of currencies\n",
    "c_list = ['c_eur', 'c_gbp', 'c_jpy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new feature for more stable forecast\n",
    "for cur in c_list:    \n",
    "    # logreturn of the means\n",
    "    df[(cur,'mean_log_r')] = np.log(1.0 + df[(cur,'mean')].pct_change())\n",
    "    df[(cur,'mean_log_r')] = df[(cur,'mean_log_r')].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "TOP COLUMN:  c_eur\n",
      "SUBCOLUMNS:  12\n",
      "['mean', 'std', 'bar_len', 'bar_spearman', 'bar_log_r', 'first_r', 'max_r', 'min_r', 'last_r', 'bar_quantile_25_r', 'bar_quantile_75_r', 'mean_log_r']\n",
      "----------------------------------\n",
      "TOP COLUMN:  c_gbp\n",
      "SUBCOLUMNS:  12\n",
      "['mean', 'std', 'bar_len', 'bar_spearman', 'bar_log_r', 'first_r', 'max_r', 'min_r', 'last_r', 'bar_quantile_25_r', 'bar_quantile_75_r', 'mean_log_r']\n",
      "----------------------------------\n",
      "TOP COLUMN:  c_jpy\n",
      "SUBCOLUMNS:  12\n",
      "['mean', 'std', 'bar_len', 'bar_spearman', 'bar_log_r', 'first_r', 'max_r', 'min_r', 'last_r', 'bar_quantile_25_r', 'bar_quantile_75_r', 'mean_log_r']\n",
      "----------------------------------\n",
      "TOP COLUMN:  month\n",
      "SUBCOLUMNS:  12\n",
      "['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10', '_11', '_12']\n",
      "----------------------------------\n",
      "TOP COLUMN:  dow\n",
      "SUBCOLUMNS:  6\n",
      "['_0', '_1', '_2', '_3', '_4', '_6']\n",
      "----------------------------------\n",
      "TOP COLUMN:  hour\n",
      "SUBCOLUMNS:  24\n",
      "['_0', '_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10', '_11', '_12', '_13', '_14', '_15', '_16', '_17', '_18', '_19', '_20', '_21', '_22', '_23']\n",
      "----------------------------------\n",
      "TOP COLUMN:  event_cur\n",
      "SUBCOLUMNS:  4\n",
      "['_EUR', '_GBP', '_JPY', '_USD']\n",
      "----------------------------------\n",
      "TOP COLUMN:  event_exist\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  actual_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  surprise_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  change_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n",
      "----------------------------------\n",
      "TOP COLUMN:  after_counter_ohe\n",
      "SUBCOLUMNS:  76\n",
      "['_eur event', '_eur speech', '_european monetary union consumer price index  core yoy', '_european monetary union consumer price index yoy', '_european monetary union ecb deposit rate decision', '_european monetary union ecb interest rate decision', '_european monetary union gross domestic product sa qoq', '_european monetary union gross domestic product sa yoy', '_european monetary union markit pmi composite', '_france markit manufacturing pmi', '_gbp event', '_gbp speech', '_germany gross domestic product qoq', '_germany harmonised index of consumer prices yoy', '_germany harmonized index of consumer prices yoy', '_germany markit manufacturing pmi', '_germany unemployment change', '_germany unemployment rate sa', '_germany zew survey  economic sentiment', '_japan boj interest rate decision', '_japan current account nsa', '_japan gross domestic product qoq', '_japan tokyo cpi ex fresh food yoy', '_jpy event', '_jpy speech', '_united kingdom average earnings excluding bonus 3moyr', '_united kingdom average earnings including bonus 3moyr', '_united kingdom boe asset purchase facility', '_united kingdom boe interest rate decision', '_united kingdom boe mpc vote cut', '_united kingdom boe mpc vote hike', '_united kingdom boe mpc vote unchanged', '_united kingdom consumer inflation expectations', '_united kingdom consumer price index yoy', '_united kingdom core consumer price index yoy', '_united kingdom gross domestic product mom', '_united kingdom gross domestic product qoq', '_united kingdom ilo unemployment rate 3m', '_united kingdom niesr gdp estimate 3m', '_united states adp employment change', '_united states average hourly earnings yoy', '_united states building permits change', '_united states building permits mom', '_united states consumer confidence', '_united states consumer price index ex food  energy mom', '_united states consumer price index ex food  energy yoy', '_united states consumer price index yoy', '_united states core personal consumption expenditure  price index mom', '_united states core personal consumption expenditure  price index yoy', '_united states core personal consumption expenditures qoq', '_united states durable goods orders', '_united states durable goods orders ex defense', '_united states durable goods orders ex transportation', '_united states existing home sales mom', '_united states fed interest rate decision', '_united states goods trade balance', '_united states gross domestic product annualized', '_united states gross domestic product price index', '_united states initial jobless claims', '_united states initial jobless claims 4week average', '_united states ism manufacturing pmi', '_united states ism nonmanufacturing pmi', '_united states ism prices paid', '_united states jolts job openings', '_united states michigan consumer sentiment index', '_united states new home sales mom', '_united states nondefense capital goods orders ex aircraft', '_united states nonfarm payrolls', '_united states retail sales control group', '_united states retail sales ex autos mom', '_united states retail sales mom', '_united states reutersmichigan consumer sentiment index', '_united states trade balance', '_united states unemployment rate', '_usd event', '_usd speech']\n"
     ]
    }
   ],
   "source": [
    "# column hierarchy\n",
    "for top in list(df.columns.get_level_values(0).unique()):\n",
    "    print('----------------------------------')\n",
    "    print('TOP COLUMN: ', top)\n",
    "    print('SUBCOLUMNS: ', len(df[top].columns))\n",
    "    print(list(df[top].columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c_eur',\n",
       " 'c_gbp',\n",
       " 'c_jpy',\n",
       " 'month',\n",
       " 'dow',\n",
       " 'hour',\n",
       " 'event_cur',\n",
       " 'event_exist',\n",
       " 'actual_ohe',\n",
       " 'surprise_ohe',\n",
       " 'change_ohe',\n",
       " 'after_counter_ohe']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_level = list(df.columns.get_level_values(0).unique())\n",
    "top_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing for Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use 2016-2018 as training data, and 2019 as validation and test data\n",
    "# problem: we have events that occure only in 2019\n",
    "# we can't normalize/standardize/scale them based on 2016-2018 data\n",
    "# we could move them to the '_(currency) event' columns, \n",
    "# but there is only a few, so here I will delete them\n",
    "\n",
    "# have we all event type in 2016-2018 yers?\n",
    "df['event_exist'].loc['2016':'2019'].any().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_european monetary union markit pmi composite',\n",
       " '_germany zew survey  economic sentiment',\n",
       " '_japan gross domestic product qoq',\n",
       " '_united states consumer price index ex food  energy mom',\n",
       " '_united states initial jobless claims 4week average',\n",
       " '_united states nondefense capital goods orders ex aircraft']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find events occuring only in 2019\n",
    "event_before_19 = df['event_exist'].loc['2016':'2019'].any()\n",
    "event_only_19 = list(event_before_19[event_before_19==False].index)\n",
    "event_only_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop events only occuring in 2019\n",
    "df.drop(event_only_19, axis=1, level=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have we all event type in 2016-2018 yers?\n",
    "df['event_exist'].loc['2016':'2019'].any().all()\n",
    "#ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use only train and validation\n",
    "# Standardizing and normalizing shifts the dataset, where +- sign can be important. Later the model will adjust this by the biases,\n",
    "# but the information is lost. It could improve the performance if we make a new feature for + and - numbers, but here I don't want to make more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman doesn't need standardization\n",
    "c_single_features = ['mean', 'std', 'bar_len']\n",
    "feature_scalers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features based on data up to 2019-01-01 00:00\n",
    "for cur in c_list:\n",
    "    for feature in c_single_features:\n",
    "        column_mean = df[cur, feature].loc['2016':'2019'].mean()\n",
    "        column_std = df[cur, feature].loc['2016':'2019'].std()\n",
    "        feature_scalers.update({(cur, feature) : [column_mean, column_std]})\n",
    "        df.loc[:,(cur, feature)] = (df[cur, feature] - column_mean)/column_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('c_eur', 'bar_len'): [412.9294628759021, 381.7685744752972],\n",
       " ('c_eur', 'mean'): [1.139248301570798, 0.0499815489615703],\n",
       " ('c_eur', 'std'): [0.00010543836669119138, 9.077071115943524e-05],\n",
       " ('c_gbp', 'bar_len'): [387.5186401063365, 303.3104183503982],\n",
       " ('c_gbp', 'mean'): [1.326336873217585, 0.06648514955410888],\n",
       " ('c_gbp', 'std'): [0.000146642848991849, 0.00015890007722327068],\n",
       " ('c_jpy', 'bar_len'): [354.61210380295637, 299.81779856625644],\n",
       " ('c_jpy', 'mean'): [0.9066954709408044, 0.03345136241766339],\n",
       " ('c_jpy', 'std'): [9.800579913539747e-05, 9.589094629696171e-05]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! later we can use them to rescale if needed\n",
    "feature_scalers # column_mean, column_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_log_r           1435.641469\n",
      "bar_log_r            1756.307094\n",
      "first_r              1079.158746\n",
      "max_r                1680.077984\n",
      "min_r                1686.788765\n",
      "last_r                960.660734\n",
      "bar_quantile_25_r     647.482120\n",
      "bar_quantile_75_r     647.172621\n",
      "dtype: float64\n",
      "mean_log_r              1.023680\n",
      "bar_log_r               8.380970\n",
      "first_r                -0.840415\n",
      "max_r                1680.077984\n",
      "min_r               -1686.788765\n",
      "last_r                  7.540555\n",
      "bar_quantile_25_r    -647.041955\n",
      "bar_quantile_75_r     646.659518\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "c_related_features = ['mean_log_r','bar_log_r','first_r','max_r','min_r','last_r','bar_quantile_25_r','bar_quantile_75_r']\n",
    "# these are related feature, we can think of them as one feature with subfeatures\n",
    "# they are on similar scale \n",
    "# and the pairs on the other side of the mean (like max_r and min_r) are almost symmetric\n",
    "# to preserve the signs we don't subtract the mean, only divide by the std\n",
    "# this way we preserve their relative size\n",
    "# *10000000 is only for easier comparison\n",
    "print(df['c_eur'][c_related_features].abs().mean()*10000000)\n",
    "print(df['c_eur'][c_related_features].mean()*10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_std = df[cur, 'bar_log_r'].loc['2016':'2019'].std()\n",
    "\n",
    "for cur in c_list:\n",
    "    for feature in c_related_features:\n",
    "        df.loc[:,(cur, feature)] = (df[cur, feature])/related_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003561751218763309"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !!! needed later to rescale outputs of log_r features\n",
    "related_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_log_r           0.403072\n",
      "bar_log_r            0.493102\n",
      "first_r              0.302985\n",
      "max_r                0.471700\n",
      "min_r                0.473584\n",
      "last_r               0.269716\n",
      "bar_quantile_25_r    0.181788\n",
      "bar_quantile_75_r    0.181701\n",
      "dtype: float64\n",
      "mean_log_r           0.000287\n",
      "bar_log_r            0.002353\n",
      "first_r             -0.000236\n",
      "max_r                0.471700\n",
      "min_r               -0.473584\n",
      "last_r               0.002117\n",
      "bar_quantile_25_r   -0.181664\n",
      "bar_quantile_75_r    0.181557\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ok, these features remained on similar scale, but the other features as well\n",
    "print(df['c_eur'][c_related_features].abs().mean())\n",
    "print(df['c_eur'][c_related_features].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_EUR    2.000000\n",
       "_GBP    2.828427\n",
       "_JPY    1.732051\n",
       "_USD    3.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'event_cur' is on similar scale, we don't touch (normalization would be better than standardization)\n",
    "df['event_cur'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling output isn't always important, \n",
    "# but if the model has different output features it helps to weight their effect on the loss function\n",
    "# or we can use multiple loss functions and manually set the weight\n",
    "df_label = df.xs('mean_log_r', axis=1, level=1, drop_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>c_eur</th>\n",
       "      <th>c_gbp</th>\n",
       "      <th>c_jpy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean_log_r</th>\n",
       "      <th>mean_log_r</th>\n",
       "      <th>mean_log_r</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ctime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:00:00</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:05:00</th>\n",
       "      <td>-0.271969</td>\n",
       "      <td>0.959393</td>\n",
       "      <td>-0.038811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:10:00</th>\n",
       "      <td>-0.986573</td>\n",
       "      <td>0.692598</td>\n",
       "      <td>-0.022978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:15:00</th>\n",
       "      <td>-0.108544</td>\n",
       "      <td>-0.684034</td>\n",
       "      <td>-0.222788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-03 22:20:00</th>\n",
       "      <td>0.306975</td>\n",
       "      <td>-0.096665</td>\n",
       "      <td>-0.160975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:35:00</th>\n",
       "      <td>-0.048386</td>\n",
       "      <td>0.197334</td>\n",
       "      <td>-0.022041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:40:00</th>\n",
       "      <td>-0.000601</td>\n",
       "      <td>1.112390</td>\n",
       "      <td>0.169539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:45:00</th>\n",
       "      <td>-0.182122</td>\n",
       "      <td>1.715093</td>\n",
       "      <td>-0.207541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:50:00</th>\n",
       "      <td>-0.151968</td>\n",
       "      <td>0.194708</td>\n",
       "      <td>0.026822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31 21:55:00</th>\n",
       "      <td>-0.198915</td>\n",
       "      <td>-0.343131</td>\n",
       "      <td>-0.608848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298829 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         c_eur      c_gbp      c_jpy\n",
       "                    mean_log_r mean_log_r mean_log_r\n",
       "ctime                                               \n",
       "2016-01-03 22:00:00   0.000000   0.000000   0.000000\n",
       "2016-01-03 22:05:00  -0.271969   0.959393  -0.038811\n",
       "2016-01-03 22:10:00  -0.986573   0.692598  -0.022978\n",
       "2016-01-03 22:15:00  -0.108544  -0.684034  -0.222788\n",
       "2016-01-03 22:20:00   0.306975  -0.096665  -0.160975\n",
       "...                        ...        ...        ...\n",
       "2019-12-31 21:35:00  -0.048386   0.197334  -0.022041\n",
       "2019-12-31 21:40:00  -0.000601   1.112390   0.169539\n",
       "2019-12-31 21:45:00  -0.182122   1.715093  -0.207541\n",
       "2019-12-31 21:50:00  -0.151968   0.194708   0.026822\n",
       "2019-12-31 21:55:00  -0.198915  -0.343131  -0.608848\n",
       "\n",
       "[298829 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should consider every event type alone, but there are 76 event types, so just normalize\n",
    "df['actual_ohe'] = (df['actual_ohe'] - df['actual_ohe'].loc['2016':'2019'].min())/(df['actual_ohe'].loc['2016':'2019'].max() - df['actual_ohe'].loc['2016':'2019'].min())\n",
    "# where max-min == 0 we get NaN, fill with 0\n",
    "df['actual_ohe'] = df['actual_ohe'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing with the max of absolute values keeps the signs\n",
    "df['surprise_ohe'] = df['surprise_ohe']/df['surprise_ohe'].loc['2016':'2019'].abs().max()\n",
    "df['surprise_ohe'] = df['surprise_ohe'].fillna(0.0)\n",
    "\n",
    "df['change_ohe'] = df['change_ohe']/df['change_ohe'].loc['2016':'2019'].abs().max()\n",
    "df['change_ohe'] = df['change_ohe'].fillna(0.0)\n",
    "\n",
    "df['after_counter_ohe'] = df['after_counter_ohe']/df['after_counter_ohe'].loc['2016':'2019'].abs().max()\n",
    "df['after_counter_ohe'] = df['after_counter_ohe'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_label = df_label.astype('float16')\n",
    "# df = df.astype('float16')\n",
    "df_valid = df.loc['2019':]\n",
    "df = df.loc[:'2019']\n",
    "df_label_valid = df_label.loc['2019':]\n",
    "df_label = df_label.loc[:'2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=288\n",
    "target_size=10 # this is defined by the network, and not the network is defined by this\n",
    "shift=1\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label data begins from the end of the first train data\n",
    "\n",
    "def ds_input(df, concat_tops, feature_to_last):\n",
    "    nx = df.to_numpy()\n",
    "    if (isinstance(df.columns, pd.core.index.MultiIndex) and concat_tops==False):\n",
    "        top_level_nb = len(df.columns.get_level_values(0).unique())\n",
    "        sub_level_nb = len(df.columns.get_level_values(1).unique())\n",
    "        nx = nx.reshape(-1,top_level_nb,sub_level_nb)\n",
    "        if feature_to_last == True:\n",
    "            nx = np.moveaxis(nx, [0,1,2], [0,2,1])\n",
    "        nx = np.squeeze(nx)\n",
    "    return nx\n",
    "\n",
    "def make_ds(batch_size, time_steps, shift, skip_steps, df, concat_tops=False, feature_to_last=True):\n",
    "    nx = ds_input(df, concat_tops, feature_to_last)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(nx[skip_steps:])\n",
    "    ds = ds.window(time_steps, shift=shift, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window: window.batch(time_steps))\n",
    "    return ds.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dataset_args = {'batch_size':batch_size,\n",
    "                      'time_steps':target_size,\n",
    "                      'shift':shift, \n",
    "                      'skip_steps':train_size}\n",
    "\n",
    "ds_label = make_ds(**label_dataset_args, df=df_label)\n",
    "#ds_label_mr = make_ds(**label_dataset_args, df=df_label, concat_tops=True)\n",
    "#ds_label_means = make_ds(**label_dataset_args, df=df_label.xs('mean', axis=1, level=1))\n",
    "\n",
    "ds_label_valid = make_ds(**label_dataset_args, df=df_label_valid)\n",
    "#ds_label_mr_valid = make_ds(**label_dataset_args, df=df_label_valid, concat_tops=True)\n",
    "#ds_label_means_valid = make_ds(**label_dataset_args, df=df_label_valid.xs('mean', axis=1, level=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dataset_args = {'batch_size':batch_size,\n",
    "                      'time_steps':train_size,\n",
    "                      'shift':shift, \n",
    "                      'skip_steps':0}\n",
    "\n",
    "ds_bars_feat_end = make_ds(**input_dataset_args, df=df[['c_eur', 'c_gbp', 'c_jpy']])\n",
    "ds_bars_curr_end = make_ds(**input_dataset_args, df=df[['c_eur', 'c_gbp', 'c_jpy']], concat_tops=False, feature_to_last=False)\n",
    "ds_bars_simple = make_ds(**input_dataset_args, df=df[['c_eur', 'c_gbp', 'c_jpy']], concat_tops=True)\n",
    "ds_event_ohe = make_ds(**input_dataset_args, df=df[['actual_ohe','surprise_ohe','change_ohe','after_counter_ohe']], concat_tops=True) # for simplicity\n",
    "ds_event_exist = make_ds(**input_dataset_args, df=df['event_exist'])\n",
    "ds_time_cur = make_ds(**input_dataset_args, df=df[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)\n",
    "\n",
    "ds_bars_feat_end_valid = make_ds(**input_dataset_args, df=df_valid[['c_eur', 'c_gbp', 'c_jpy']])\n",
    "ds_bars_curr_end_valid = make_ds(**input_dataset_args, df=df_valid[['c_eur', 'c_gbp', 'c_jpy']], concat_tops=False, feature_to_last=False)\n",
    "ds_bars_simple_valid = make_ds(**input_dataset_args, df=df_valid[['c_eur', 'c_gbp', 'c_jpy']], concat_tops=True)\n",
    "ds_event_ohe_valid = make_ds(**input_dataset_args, df=df_valid[['actual_ohe','surprise_ohe','change_ohe','after_counter_ohe']], concat_tops=True) # for simplicity\n",
    "ds_event_exist_valid = make_ds(**input_dataset_args, df=df_valid['event_exist'])\n",
    "ds_time_cur_valid = make_ds(**input_dataset_args, df=df_valid[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dataset_args = {'batch_size':batch_size,\n",
    "                      'time_steps':target_size,\n",
    "                      'shift':shift, \n",
    "                      'skip_steps':train_size}\n",
    "\n",
    "ds_event_future = make_ds(**future_dataset_args, df=df['event_exist'])\n",
    "ds_time_curr_future = make_ds(**future_dataset_args, df=df[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)\n",
    "\n",
    "ds_event_future_valid = make_ds(**future_dataset_args, df=df_valid['event_exist'])\n",
    "ds_time_curr_future_valid = make_ds(**future_dataset_args, df=df_valid[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_label  <BatchDataset shapes: (64, None, 3), types: tf.float64>\n",
      "ds_bars_feat_end  <BatchDataset shapes: (64, None, 12, 3), types: tf.float64>\n",
      "ds_bars_curr_end  <BatchDataset shapes: (64, None, 3, 12), types: tf.float64>\n",
      "ds_bars_simple  <BatchDataset shapes: (64, None, 36), types: tf.float64>\n",
      "ds_event_ohe  <BatchDataset shapes: (64, None, 280), types: tf.float64>\n",
      "ds_time_cur <BatchDataset shapes: (64, None, 46), types: tf.float64>\n",
      "ds_event_exist  <BatchDataset shapes: (64, None, 70), types: tf.float64>\n",
      "ds_event_future  <BatchDataset shapes: (64, None, 70), types: tf.float64>\n",
      "ds_time_curr_future  <BatchDataset shapes: (64, None, 46), types: tf.float64>\n"
     ]
    }
   ],
   "source": [
    "print('ds_label ',ds_label)\n",
    "\n",
    "print('ds_bars_feat_end ',ds_bars_feat_end) # not used\n",
    "print('ds_bars_curr_end ',ds_bars_curr_end) # not used\n",
    "print('ds_bars_simple ',ds_bars_simple)\n",
    "print('ds_event_ohe ', ds_event_ohe) \n",
    "# more than one ocasion per number => no entity embedding\n",
    "print('ds_time_cur', ds_time_cur) \n",
    "print('ds_event_exist ', ds_event_exist)\n",
    "# \n",
    "print('ds_event_future ', ds_event_future)\n",
    "print('ds_time_curr_future ', ds_time_curr_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedActivationUnit(keras.layers.Layer):\n",
    "    def __init__(self, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'activation': self.activation,\n",
    "        })\n",
    "        return config \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        n_filters = inputs.shape[-1] // 2\n",
    "        linear_output = self.activation(inputs[..., :n_filters])\n",
    "        gate = keras.activations.sigmoid(inputs[..., n_filters:])\n",
    "        return self.activation(linear_output) * gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavenet_residual_block(inputs, n_filters, dilation_rate):\n",
    "    z = keras.layers.Conv1D(2 * n_filters, kernel_size=2, padding=\"causal\",\n",
    "                            dilation_rate=dilation_rate)(inputs)\n",
    "    z = GatedActivationUnit()(z)\n",
    "    z = keras.layers.Conv1D(n_filters, kernel_size=1)(z)\n",
    "    return keras.layers.Add()([z, inputs]), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "input_c1 = keras.layers.Input(shape=[train_size, 12, 3], name = 'input_c1')\n",
    "input_c2 = keras.layers.Input(shape=[train_size, 3, 12], name = 'input_c2')\n",
    "input_simple_c = keras.layers.Input(shape=[train_size, 36], name = 'input_simple_c')\n",
    "input_ohe = keras.layers.Input(shape=[train_size, 280], name = 'input_ohe')\n",
    "input_time_cur = keras.layers.Input(shape=[train_size, 46], name = 'input_time_cur')\n",
    "input_event_exist = keras.layers.Input(shape=[train_size, 70], name = 'input_event_exist')\n",
    "\n",
    "input_f_time_cur = keras.layers.Input(shape=[target_size, 46], name = 'input_f_time_cur')\n",
    "input_f_event_exist = keras.layers.Input(shape=[target_size, 70], name = 'input_f_event_exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_event_exist (InputLaye [(None, 288, 70)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 288, 10)           710       \n",
      "=================================================================\n",
      "Total params: 710\n",
      "Trainable params: 710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_event_exist_setup(inputs, embed_dim, name=None):\n",
    "    out = keras.layers.Conv1D(embed_dim, kernel_size=1, strides=1, padding='valid')(inputs)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "model_event_exist = model_event_exist_setup(input_event_exist, 10) #, name='mod_event_exist')\n",
    "# model_event_exist_f = model_event_exist_setup(input_f_event_exist, 10, name='mod_event_exist_f')\n",
    "model_event_exist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_time_cur (InputLayer)  [(None, 288, 46)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 288, 6)            282       \n",
      "=================================================================\n",
      "Total params: 282\n",
      "Trainable params: 282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_time_cur_setup(inputs, embed_dim, name=None):\n",
    "    out = keras.layers.Conv1D(embed_dim, kernel_size=1, strides=1, padding='valid')(inputs)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "model_time_cur = model_time_cur_setup(input_time_cur, 6) #, name='mod_time_cur')\n",
    "# model_time_cur_f = model_time_cur_setup(input_f_time_cur, 6, name='mod_time_cur_f')\n",
    "model_time_cur.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ohe (InputLayer)       [(None, 288, 280)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 288, 16)           4496      \n",
      "=================================================================\n",
      "Total params: 4,496\n",
      "Trainable params: 4,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def model_event_ohe_setup(inputs, embed_dim): \n",
    "    out = keras.layers.Conv1D(embed_dim, kernel_size=1, strides=1, padding='valid')(inputs)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out]) #, name = 'mod_event_ohe')\n",
    "\n",
    "model_event_ohe = model_event_ohe_setup(input_ohe, 16)\n",
    "model_event_ohe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mod_wavenet_fl32_fd65\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 288, 68)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 288, 64)      8768        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 288, 128)     16512       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit (GatedAct (None, 288, 64)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 288, 64)      4160        gated_activation_unit[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 288, 64)      0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 288, 128)     16512       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_1 (GatedA (None, 288, 64)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 288, 64)      4160        gated_activation_unit_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 288, 64)      0           conv1d_9[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 288, 128)     16512       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_2 (GatedA (None, 288, 64)      0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 288, 64)      4160        gated_activation_unit_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 288, 64)      0           conv1d_11[0][0]                  \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 288, 128)     16512       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_3 (GatedA (None, 288, 64)      0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 288, 64)      4160        gated_activation_unit_3[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 288, 64)      0           conv1d_13[0][0]                  \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 288, 128)     16512       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_4 (GatedA (None, 288, 64)      0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 288, 64)      4160        gated_activation_unit_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 288, 64)      0           conv1d_15[0][0]                  \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 288, 128)     16512       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gated_activation_unit_5 (GatedA (None, 288, 64)      0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 288, 64)      4160        gated_activation_unit_5[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 288, 64)      0           conv1d_7[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_13[0][0]                  \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Relu (TensorFlowOpL [(None, 288, 64)]    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 288, 64)      4160        tf_op_layer_Relu[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 288, 64)      4160        conv1d_18[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 141,120\n",
      "Trainable params: 141,120\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def wavenet_model_setup(n_layers_per_block, n_blocks, n_filters, n_outputs, feature_dim, name):\n",
    "    # n_layers_per_block = 10 in the paper\n",
    "    # n_blocks = 3 in the paper\n",
    "    # n_filters = 128 in the paper\n",
    "    # n_outputs = 256 in the paper\n",
    "    \n",
    "    inputs = keras.layers.Input(shape=[train_size, feature_dim]) # 164 with cross model\n",
    "    z = keras.layers.Conv1D(n_filters, kernel_size=2, padding=\"causal\")(inputs)\n",
    "    skip_to_last = []\n",
    "    for dilation_rate in [2**i for i in range(n_layers_per_block)] * n_blocks:\n",
    "        z, skip = wavenet_residual_block(z, n_filters, dilation_rate)\n",
    "        skip_to_last.append(skip)\n",
    "    z = keras.activations.relu(keras.layers.Add()(skip_to_last))\n",
    "    z = keras.layers.Conv1D(n_filters, kernel_size=1, activation=\"relu\")(z)\n",
    "    Y_proba = keras.layers.Conv1D(n_outputs, kernel_size=1, activation=\"softmax\")(z)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[Y_proba], name = name)\n",
    "\n",
    "model_wn = wavenet_model_setup(n_layers_per_block=3, n_blocks=2, n_filters=64, n_outputs=64, feature_dim=68, name='mod_wavenet_fl32_fd65')\n",
    "# (fast test) #  model_wn = wavenet_model(n_layers_per_block=2, n_blocks=1, n_filters=1, n_outputs=3)\n",
    "\n",
    "#keras.utils.plot_model(model, show_shapes=True)\n",
    "model_wn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 288, 64)]         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 288, 64)           256       \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 95, 8)             2568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 95, 8)             32        \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 31, 16)            400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 31, 16)            64        \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 10, 32)            1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 10, 32)            128       \n",
      "=================================================================\n",
      "Total params: 5,016\n",
      "Trainable params: 4,776\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def time_transformer_model_setup(input_dim, output_dim):\n",
    "    inputs = keras.layers.Input(shape=[train_size, input_dim])\n",
    "    z = keras.layers.BatchNormalization()(inputs)\n",
    "    z = keras.layers.Conv1D(8, kernel_size=5, strides=3, padding='valid', activation='relu')(z)\n",
    "    z = keras.layers.BatchNormalization()(z)\n",
    "    z = keras.layers.Conv1D(16, kernel_size=3, strides=3, padding='valid', activation='relu')(z)\n",
    "    z = keras.layers.BatchNormalization()(z)\n",
    "    z = keras.layers.Conv1D(output_dim, kernel_size=3, strides=3, padding='valid', activation='relu')(z)\n",
    "    out = keras.layers.BatchNormalization()(z)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out])\n",
    "\n",
    "model_time_transformer = time_transformer_model_setup(64, 32)\n",
    "model_time_transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Simplest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_metric(time_start=None, time_end=None, feat_start=None, feat_end=None):\n",
    "    def mae_segment(y_true, y_pred):\n",
    "        return tf.abs(y_true[:, time_start:time_end, feat_start:feat_end] - y_pred[:, time_start:time_end, feat_start:feat_end])\n",
    "    return mae_segment\n",
    "\n",
    "def mae_mean_full(y_true, y_pred):\n",
    "    return tf.abs(y_true[:, :, :3] - y_pred[:, :, :3])\n",
    "\n",
    "def mae_mean_first(y_true, y_pred):\n",
    "    return tf.abs(y_true[:, 0, :3] - y_pred[:, 0, :3])\n",
    "\n",
    "def weighted_mae_loss_setup(steps):\n",
    "    def weighted_mae_loss(y_true, y_pred):\n",
    "        i_divider = tf.math.sqrt(tf.range(1, steps+1, dtype='float32'))\n",
    "        loss = tf.abs(y_true - y_pred)\n",
    "        return tf.math.divide(loss, i_divider[:,None])\n",
    "    return weighted_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_time_cur:0\", shape=(None, 288, 46), dtype=float32) for input (None, 288, 46), but it was re-called on a Tensor with incompatible shape (None, 10, 46).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_event_exist:0\", shape=(None, 288, 70), dtype=float32) for input (None, 288, 70), but it was re-called on a Tensor with incompatible shape (None, 10, 70).\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ohe (InputLayer)          [(None, 288, 280)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_time_cur (InputLayer)     [(None, 288, 46)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_event_exist (InputLayer)  [(None, 288, 70)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_simple_c (InputLayer)     [(None, 288, 36)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 (None, 288, 16)      4496        input_ohe[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 multiple             282         input_time_cur[0][0]             \n",
      "                                                                 input_f_time_cur[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 multiple             710         input_event_exist[0][0]          \n",
      "                                                                 input_f_event_exist[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 288, 68)      0           input_simple_c[0][0]             \n",
      "                                                                 model_3[1][0]                    \n",
      "                                                                 model_2[1][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mod_wavenet_fl32_fd65 (Model)   (None, 288, 64)      141120      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_f_time_cur (InputLayer)   [(None, 10, 46)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_f_event_exist (InputLayer [(None, 10, 70)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_4 (Model)                 (None, 10, 32)       5016        mod_wavenet_fl32_fd65[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 48)       0           model_4[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "                                                                 model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 10, 24)       1176        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10, 24)       96          conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 10, 3)        75          batch_normalization[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 152,971\n",
      "Trainable params: 152,683\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ohe = model_event_ohe(input_ohe)\n",
    "time_cur = model_time_cur(input_time_cur)\n",
    "event_ex = model_event_exist(input_event_exist)\n",
    "\n",
    "inputs = keras.layers.Concatenate(axis=-1)([input_simple_c, ohe, time_cur, event_ex])\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_time_transformer(wave)\n",
    "\n",
    "time_cur_f = model_time_cur(input_f_time_cur)\n",
    "event_ex_f = model_event_exist(input_f_event_exist)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, time_cur_f, event_ex_f])\n",
    "input_f = keras.layers.Conv1D(24, kernel_size=1, strides=1, padding='valid', activation='relu')(input_f)\n",
    "input_f = keras.layers.BatchNormalization()(input_f)\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid', activation=None)(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_simple_c, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label_means)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip(((ds_bars_simple_valid, ds_event_ohe_valid, ds_time_cur_valid, ds_event_exist_valid, ds_time_curr_future_valid, ds_event_future_valid), \n",
    "                                 ds_label_means_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_event_exist:0\", shape=(None, 288, 70), dtype=float32) for input (None, 288, 70), but it was re-called on a Tensor with incompatible shape (64, 10, 70).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_time_cur:0\", shape=(None, 288, 46), dtype=float32) for input (None, 288, 46), but it was re-called on a Tensor with incompatible shape (64, 10, 46).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_event_exist:0\", shape=(None, 288, 70), dtype=float32) for input (None, 288, 70), but it was re-called on a Tensor with incompatible shape (64, 10, 70).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_time_cur:0\", shape=(None, 288, 46), dtype=float32) for input (None, 288, 46), but it was re-called on a Tensor with incompatible shape (64, 10, 46).\n",
      "   3498/Unknown - 660s 189ms/step - loss: 0.2229 - mae: 0.4997 - mae_mean_first: 0.5006 - mae_mean_full: 0.4997WARNING:tensorflow:Model was constructed with shape Tensor(\"input_event_exist:0\", shape=(None, 288, 70), dtype=float32) for input (None, 288, 70), but it was re-called on a Tensor with incompatible shape (64, 10, 70).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_time_cur:0\", shape=(None, 288, 46), dtype=float32) for input (None, 288, 46), but it was re-called on a Tensor with incompatible shape (64, 10, 46).\n",
      "3498/3498 [==============================] - 869s 248ms/step - loss: 0.2229 - mae: 0.4997 - mae_mean_first: 0.5006 - mae_mean_full: 0.4997 - val_loss: 0.1303 - val_mae: 0.3570 - val_mae_mean_first: 0.3572 - val_mae_mean_full: 0.3570\n",
      "Epoch 2/14\n",
      "3498/3498 [==============================] - 871s 249ms/step - loss: 0.2189 - mae: 0.4940 - mae_mean_first: 0.4942 - mae_mean_full: 0.4940 - val_loss: 1.5218 - val_mae: 1.9506 - val_mae_mean_first: 2.0341 - val_mae_mean_full: 1.9506\n",
      "Epoch 3/14\n",
      "3498/3498 [==============================] - 891s 255ms/step - loss: 0.2189 - mae: 0.4939 - mae_mean_first: 0.4941 - mae_mean_full: 0.4939 - val_loss: 0.1310 - val_mae: 0.3586 - val_mae_mean_first: 0.3577 - val_mae_mean_full: 0.3586\n",
      "Epoch 4/14\n",
      "3498/3498 [==============================] - 868s 248ms/step - loss: 0.2188 - mae: 0.4938 - mae_mean_first: 0.4949 - mae_mean_full: 0.4938 - val_loss: 0.3749 - val_mae: 0.7824 - val_mae_mean_first: 0.7827 - val_mae_mean_full: 0.7824\n",
      "Epoch 5/14\n",
      "3498/3498 [==============================] - 866s 248ms/step - loss: 0.2191 - mae: 0.4942 - mae_mean_first: 0.4951 - mae_mean_full: 0.4942 - val_loss: 0.3442 - val_mae: 0.7183 - val_mae_mean_first: 0.7184 - val_mae_mean_full: 0.7183\n",
      "Epoch 6/14\n",
      "3498/3498 [==============================] - 855s 244ms/step - loss: 0.2195 - mae: 0.4951 - mae_mean_first: 0.4954 - mae_mean_full: 0.4951 - val_loss: 3385.6087 - val_mae: 3386.1003 - val_mae_mean_first: 3386.0994 - val_mae_mean_full: 3386.1177\n",
      "Epoch 7/14\n",
      "3498/3498 [==============================] - 795s 227ms/step - loss: 0.2190 - mae: 0.4942 - mae_mean_first: 0.4941 - mae_mean_full: 0.4942 - val_loss: 58373.9410 - val_mae: 58374.3984 - val_mae_mean_first: 58373.7461 - val_mae_mean_full: 58375.1992\n",
      "Epoch 8/14\n",
      "3498/3498 [==============================] - 870s 249ms/step - loss: 0.2188 - mae: 0.4940 - mae_mean_first: 0.4939 - mae_mean_full: 0.4940 - val_loss: 49078.0811 - val_mae: 49078.5938 - val_mae_mean_first: 49077.9219 - val_mae_mean_full: 49079.1445\n",
      "Epoch 9/14\n",
      "3498/3498 [==============================] - 850s 243ms/step - loss: 0.2188 - mae: 0.4938 - mae_mean_first: 0.4938 - mae_mean_full: 0.4938 - val_loss: 25081.5722 - val_mae: 25081.8730 - val_mae_mean_first: 25082.3770 - val_mae_mean_full: 25081.8066\n",
      "Epoch 10/14\n",
      "3498/3498 [==============================] - 837s 239ms/step - loss: 0.2187 - mae: 0.4938 - mae_mean_first: 0.4937 - mae_mean_full: 0.4938 - val_loss: 26379.3987 - val_mae: 26379.9023 - val_mae_mean_first: 26379.7422 - val_mae_mean_full: 26380.1016\n",
      "Epoch 11/14\n",
      "2801/3498 [=======================>......] - ETA: 2:06 - loss: 0.2332 - mae: 0.5133 - mae_mean_first: 0.5133 - mae_mean_full: 0.5133WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-ec64b760eabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m history = model_sum.fit(ds_train, epochs=14,\n\u001b[1;32m      6\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                        callbacks=[checkpoint_cb])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_sum.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\", mae_mean_first, mae_mean_full])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_simple.h5\", save_best_only=True)\n",
    "\n",
    "history = model_sum.fit(ds_train, epochs=14,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with price and return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Concatenate(axis=-1)([input_simple_c, \n",
    "                                            model_event_ohe(input_ohe), \n",
    "                                            model_time_cur(input_time_cur), \n",
    "                                            model_event_exist(input_event_exist)])\n",
    "\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_time_transformer(wave)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, \n",
    "                                             model_time_cur_f(input_f_time_cur), \n",
    "                                             model_event_exist_f(input_f_event_exist)])\n",
    "\n",
    "input_f = keras.layers.Conv1D(24, kernel_size=1, strides=1, padding='valid', activation='relu')(input_f)\n",
    "input_f = keras.layers.BatchNormalization()(input_f)\n",
    "out_f = keras.layers.Conv1D(6, kernel_size=1, strides=1, padding='valid', activation=None)(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_simple_c, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label_mr)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip(((ds_bars_simple_valid, ds_event_ohe_valid, ds_time_cur_valid, ds_event_exist_valid, ds_time_curr_future_valid, ds_event_future_valid), \n",
    "                                 ds_label_mr_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model_sum.compile(loss=weighted_mae_loss_setup(target_size), optimizer=optimizer, metrics=[\"mae\", mae_mean_hour, mae_mean_full])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_out_mr.h5\", save_best_only=True)\n",
    "\n",
    "history = model_sum.fit(ds_train, epochs=14,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_time_cur_1:0\", shape=(None, 288, 46), dtype=float32) for input (None, 288, 46), but it was re-called on a Tensor with incompatible shape (None, 10, 46).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_event_exist_1:0\", shape=(None, 288, 70), dtype=float32) for input (None, 288, 70), but it was re-called on a Tensor with incompatible shape (None, 10, 70).\n"
     ]
    }
   ],
   "source": [
    "model_direct = keras.models.load_model(\"model_wn_simple.h5\", custom_objects={'GatedActivationUnit' : GatedActivationUnit, 'mae_mean_full' : mae_mean_full, 'mae_mean_first' : mae_mean_first})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_direct.evaluate(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_event_exist:0\", shape=(None, 288, 70), dtype=float32) for input (None, 288, 70), but it was re-called on a Tensor with incompatible shape (64, 10, 70).\n",
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_time_cur:0\", shape=(None, 288, 46), dtype=float32) for input (None, 288, 46), but it was re-called on a Tensor with incompatible shape (64, 10, 46).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-73a198fcc97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred = model_sum.predict(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Simple with decoupled features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label_means)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip(((ds_bars_simple_valid, ds_event_ohe_valid, ds_time_cur_valid, ds_event_exist_valid, ds_time_curr_future_valid, ds_event_future_valid), \n",
    "                                 ds_label_means_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future and past embeds are different\n",
    "model_time_cur_f = model_time_cur_setup(input_f_time_cur, 6, name='model_time_cur_f')\n",
    "model_event_exist_f = model_event_exist_setup(input_f_event_exist, 10, name='model_event_exist_f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = model_event_ohe(input_ohe)\n",
    "time_cur = model_time_cur(input_time_cur)\n",
    "event_ex = model_event_exist(input_event_exist)\n",
    "inputs = keras.layers.Concatenate(axis=-1)([input_simple_c, ohe, time_cur, event_ex])\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_time_transform(wave)\n",
    "\n",
    "time_cur_f = model_time_cur_f(input_f_time_cur)\n",
    "event_ex_f = model_event_exist_f(input_f_event_exist)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, time_cur_f, event_ex_f])\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid')(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_simple_c, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model_sum.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\", mean_metric(last_index=None)])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_decoup.h5\", save_best_only=True)\n",
    "\n",
    "history = model_sum.fit(ds_train, epochs=14,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefine Head Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_model2_setup():\n",
    "    inputs = keras.layers.Input(shape=[train_size, 3])\n",
    "    z = keras.layers.BatchNormalization()(inputs)\n",
    "    z = keras.layers.Conv1D(8, kernel_size=5, strides=3, padding='valid', activation='relu')(z)\n",
    "    z = keras.layers.BatchNormalization()(z)\n",
    "    z = keras.layers.Conv1D(16, kernel_size=3, strides=2, padding='valid', activation='relu')(z)\n",
    "    z = keras.layers.BatchNormalization()(z)\n",
    "    z = keras.layers.Conv1D(32, kernel_size=3, strides=2, padding='valid', activation='relu')(z)\n",
    "    out = keras.layers.BatchNormalization()(z)\n",
    "    return keras.models.Model(inputs=[inputs], outputs=[out], name = 'model_head2')\n",
    "\n",
    "model_head2 = head_model2_setup()\n",
    "model_head2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck?\n",
    "model_event_ohe = model_event_ohe_setup(input_ohe, 64)\n",
    "model_event_ohe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +48 features\n",
    "model_wn = wavenet_model_setup(n_layers_per_block=3, n_blocks=2, n_filters=64, n_outputs=3, feature_dim=113, name='model_wn_fl64_fd81')\n",
    "# model_wn_out = wavenet_model_setup(n_layers_per_block=3, n_blocks=1, n_filters=64, n_outputs=3, feature_dim=48, name='model_wn_out_fl64_fd_48')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size=71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_label = make_ds(batch_size=batch_size,\n",
    "                   time_steps=target_size, \n",
    "                   shift=shift, \n",
    "                   skip_steps=train_size, \n",
    "                   df=df_label)\n",
    "\n",
    "ds_label_valid = make_ds(batch_size=batch_size,\n",
    "                   time_steps=target_size, \n",
    "                   shift=shift, \n",
    "                   skip_steps=train_size, \n",
    "                   df=df_label_valid)\n",
    "ds_event_future = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df['event_exist'])\n",
    "ds_time_curr_future = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)\n",
    "\n",
    "ds_event_future_valid = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df_valid['event_exist'])\n",
    "ds_time_curr_future_valid = make_ds(  batch_size=batch_size,\n",
    "                            time_steps=target_size, \n",
    "                            shift=shift, \n",
    "                            skip_steps=train_size, \n",
    "                            df=df_valid[['month', 'dow', 'hour', 'event_cur']], concat_tops=True)\n",
    "\n",
    "input_f_time_cur = keras.layers.Input(shape=[target_size, 46], name = 'input_f_time_cur')\n",
    "input_f_event_exist = keras.layers.Input(shape=[target_size, 70], name = 'input_f_event_exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = model_event_ohe(input_ohe)\n",
    "time_cur = model_time_cur(input_time_cur)\n",
    "event_ex = model_event_exist(input_event_exist)\n",
    "inputs = keras.layers.Concatenate(axis=-1)([input_simple_c, ohe, time_cur, event_ex])\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_head2(wave)\n",
    "\n",
    "time_cur_f = model_time_cur(input_f_time_cur)\n",
    "event_ex_f = model_event_exist(input_f_event_exist)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, time_cur_f, event_ex_f])\n",
    "input_f = keras.layers.Conv1D(48, kernel_size=3, strides=1, padding='causal', activation='relu')(input_f)\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid')(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_simple_c, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if model converge\n",
    "'''\n",
    "ds_worker_test = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label)).shuffle(64).prefetch(tf.data.experimental.AUTOTUNE).take(64)\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model_sum.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "history = model_sum.fit(ds_worker_test, epochs=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.zip(((ds_bars_simple, ds_event_ohe, ds_time_cur, ds_event_exist, ds_time_curr_future, ds_event_future), \n",
    "                                 ds_label)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_valid = tf.data.Dataset.zip(((ds_bars_simple_valid, ds_event_ohe_valid, ds_time_cur_valid, ds_event_exist_valid, ds_time_curr_future_valid, ds_event_future_valid), \n",
    "                                 ds_label_valid)).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "model_sum.compile(loss=keras.losses.Huber(), optimizer=optimizer, metrics=[\"mae\"])\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model_wn_64_head_2.h5\", save_best_only=True)\n",
    "\n",
    "history = model_sum.fit(ds_train, epochs=20,\n",
    "                       validation_data=ds_valid,\n",
    "                       callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Cross Feature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "z1 = keras.layers.Conv2D(2*3, kernel_size=1, padding=\"valid\")(input_c1)\n",
    "z1 = keras.layers.Reshape([train_size,66])(z1)\n",
    "\n",
    "z2 = keras.layers.Conv2D(2*11, kernel_size=1, padding=\"valid\")(input_c2)\n",
    "z2 = keras.layers.Reshape([train_size,66])(z2)\n",
    "\n",
    "out = keras.layers.Concatenate(axis=-1)([z1, z2])\n",
    "model_cur_cross_in = keras.models.Model(inputs=[input_c1, input_c2], outputs=[out])\n",
    "model_cur_cross_in._name = 'model_cur_cross_in'\n",
    "\n",
    "model_cur_cross_in.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wn = wavenet_model(n_layers_per_block=3, n_blocks=2, n_filters=256, n_outputs=3, feature_dim=164)\n",
    "# (fast test) #  model_wn = wavenet_model(n_layers_per_block=2, n_blocks=1, n_filters=1, n_outputs=3)\n",
    "model_wn._name = 'model_wavenet'\n",
    "\n",
    "#keras.utils.plot_model(model, show_shapes=True)\n",
    "model_wn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = model_event_ohe(input_ohe)\n",
    "time_cur = model_time_cur(input_time_cur)\n",
    "event_ex = model_event_exist(input_event_exist)\n",
    "cur_cross = model_cur_cross_in([input_c1, input_c2])\n",
    "inputs = keras.layers.Concatenate(axis=-1)([cur_cross, ohe, time_cur, event_ex])\n",
    "wave = model_wn(inputs)\n",
    "out_head = model_head(wave)\n",
    "\n",
    "time_cur_f = model_time_cur(input_f_time_cur)\n",
    "event_ex_f = model_event_exist(input_f_event_exist)\n",
    "\n",
    "input_f = keras.layers.Concatenate(axis=-1)([out_head, time_cur_f, event_ex_f])\n",
    "out_f = keras.layers.Conv1D(3, kernel_size=1, strides=1, padding='valid')(input_f)\n",
    "\n",
    "model_sum = keras.models.Model(inputs=[input_c1, input_c2, input_ohe, input_time_cur, input_event_exist, input_f_time_cur, input_f_event_exist], outputs=[out_f])\n",
    "\n",
    "model_sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = keras.layers.Conv1D(64, kernel_size=1, strides=1, padding='valid')(input_ohe)\n",
    "model_event_ohe = keras.models.Model(inputs=[input_ohe], outputs=[out])\n",
    "model_event_ohe._name = 'model_event_ohe'\n",
    "\n",
    "model_event_ohe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_transform = dict()\n",
    "# standardize features based on data up to 2019-01-01 00:00\n",
    "for cur in c_list:\n",
    "    mean_to_19 = df_label[cur, 'mean'].loc['2016':'2019'].mean()\n",
    "    std_to_19 = df_label[cur, 'mean'].loc['2016':'2019'].std()\n",
    "    label_transform.update({(cur, 'mean') : {'column_mean' : mean_to_19, 'column_std' : std_to_19}})\n",
    "    #df_label.loc[:,(cur, 'mean')] = (df_label.loc[:,(cur, 'mean')] - mean_to_19)/std_to_19"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
